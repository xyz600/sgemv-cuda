                //
                // Generated by NVIDIA NVVM Compiler
                //
                // Compiler Build ID: CL-26907403
                // Cuda compilation tools, release 10.1, V10.1.243
                // Based on LLVM 3.4svn
                //

.version 6.4
.target sm_61
.address_size 64

	                // .globl	_Z9sgemv_devPKfS0_Pfi

.visible .entry _Z9sgemv_devPKfS0_Pfi(
	.param .u64 _Z9sgemv_devPKfS0_Pfi_param_0,
	.param .u64 _Z9sgemv_devPKfS0_Pfi_param_1,
	.param .u64 _Z9sgemv_devPKfS0_Pfi_param_2,
	.param .u32 _Z9sgemv_devPKfS0_Pfi_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<34>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd6, [_Z9sgemv_devPKfS0_Pfi_param_0];   // %rd6 = matrix
	ld.param.u64 	%rd7, [_Z9sgemv_devPKfS0_Pfi_param_1];   // %rd7 = vector
	ld.param.u64 	%rd8, [_Z9sgemv_devPKfS0_Pfi_param_2];   // %rd8 = result
	ld.param.u32 	%r9, [_Z9sgemv_devPKfS0_Pfi_param_3];    // %r9  = size
	mov.u32 	%r1, %ntid.y;                           // %r1  = blockDim.y
	mov.u32 	%r10, %ctaid.y;                         // %r10 = blockIdx.y
	mov.u32 	%r11, %tid.y;                           // %r11 = threadIdx.y
	mad.lo.s32 	%r12, %r1, %r10, %r11;                  // %r12 = blockIdx.y * blockDim.y + threadIdx.y
	shl.b32 	%r25, %r12, 2;                          // %r25 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size
	setp.ge.s32	%p1, %r25, %r9;                         // %p1 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size < size
	@%p1 bra 	BB0_5;

	mov.u32 	%r13, %nctaid.y;                        // %r13 = gridDim.y
	mul.lo.s32 	%r14, %r1, %r13;                        // %r14 = blockDim.y * gridDim.y
	shl.b32 	%r3, %r14, 2;                           // %r3  = blockDim.y * gridDim.y * unroll_size
	mul.lo.s32 	%r15, %r3, %r9;                         // %r15 = blockDim.y * gridDim.y * unroll_size * size 
	cvt.u64.u32	%rd1, %r15;                             // %rd1 = u32 -> u64(blockDim.y * gridDim.y * unroll_size * size)
	mov.u32 	%r16, %ntid.x;                          // %r16 = blockDim.x
	mov.u32 	%r17, %ctaid.x;                         // %r17 = blockIdx.x
	mov.u32 	%r18, %tid.x;                           // %r18 = threadIdx.x
	mad.lo.s32 	%r4, %r16, %r17, %r18;                  // %r4 = blockDim.x * blockIdx.x + threadIdx.x
	cvta.to.global.u64 	%rd9, %rd6;                     // %rd9 = to_global(matrix)
	mul.lo.s32 	%r19, %r25, %r9;                        // %r19 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size * size
	mul.wide.u32 	%rd10, %r19, 4;                     // %rd10 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size * size * sizeof(float)
			                                            //     - %rd10 = 64bit, %r19, 4 = 32bit なので .wide が付く
	add.s64 	%rd28, %rd9, %rd10;                     // %rd28 = matrix + (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size * size * sizeof(float)
										                //     - %rd28 = a
	cvta.to.global.u64 	%rd3, %rd7;                     // %rd3 = to_global(vector)
	cvta.to.global.u64 	%rd18, %rd8;                    // %rd18 = to_global(result)
	shl.b64 	%rd27, %rd1, 2;                         // %rd27 = blockDim.y * gridDim.y * unroll_size * size * sizeof(float)

BB0_2:
	mov.f32 	%f30, 0f00000000;                      // %f30 = 0.0
	setp.ge.s32	%p2, %r4, %r9;                         // p2 = (blockDim.x * blockIdx.x + threadIdx.x) >= size
	mov.f32 	%f31, %f30;                            // %f31 = 0.0
	mov.f32 	%f32, %f30;                            // %f32 = 0.0
	mov.f32 	%f33, %f30;                            // %f33 = 0.0
	mov.u32 	%r26, %r4;                             // %r26 = blockDim.x * blockIdx.x + threadIdx.x
	@%p2 bra 	BB0_4;                                 // for x loop が終了したら BB0_4

BB0_3:
	mul.wide.s32 	%rd11, %r26, 4;                    // %rd11 = blockDim.x * blockIdx.x + threadIdx.x * sizeof(float)
	add.s64 	%rd12, %rd3, %rd11;                    // %rd12 = &vector[blockDim.x * blockIdx.x + threadIdx.x]
	add.s64 	%rd13, %rd28, %rd11;                   // %rd13 = &a[blockDim.x * blockIdx.x + threadIdx.x]
	ld.global.nc.f32 	%f21, [%rd13];                 // %f21 = vector[blockDim.x * blockIdx.x + threadIdx.x]  
	ld.global.nc.f32 	%f22, [%rd12];                 // %f22 = a[blockDim.x * blockIdx.x + threadIdx.x]
	fma.rn.f32 	%f33, %f21, %f22, %f33;                // %f33 += %f21 * %f22
	mul.wide.s32 	%rd14, %r9, 4;                     // %rd14 = size * sizeof(float)
	add.s64 	%rd15, %rd13, %rd14;                   // %rd15 = &a[blockDim.x * blockIdx.x + threadIdx.x] + size
	ld.global.nc.f32 	%f23, [%rd15];                 // 
	fma.rn.f32 	%f32, %f23, %f22, %f32;                // 
	add.s64 	%rd16, %rd15, %rd14;                   // %rd16 = &a[blockDim.x * blockIdx.x + threadIdx.x] + 2 * size
	ld.global.nc.f32 	%f24, [%rd16];                 // 
	fma.rn.f32 	%f31, %f24, %f22, %f31;                // 
	add.s64 	%rd17, %rd16, %rd14;                   // %rd17 = &a[blockDim.x * blockIdx.x + threadIdx.x] + 3 * size
	ld.global.nc.f32 	%f25, [%rd17];                 // 
	fma.rn.f32 	%f30, %f25, %f22, %f30;                // 
	mov.u32 	%r21, %nctaid.x;                       // %r21 = gridDim.x 
	mad.lo.s32 	%r26, %r21, %r16, %r26;                // %r26 += gridDim.x * blockDim.x
	setp.lt.s32	%p3, %r26, %r9;                        // p3 = %r26 < size
	@%p3 bra 	BB0_3;                                 // if p3 goto BB0_3

BB0_4:
	mul.wide.s32 	%rd19, %r25, 4;                    // %rd19 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size * sizeof(float)
	add.s64 	%rd20, %rd18, %rd19;                   // %rd20 = result += (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size
	atom.global.add.f32 	%f26, [%rd20], %f33;       // %f26  = atomicAdd(result + %f33)
	add.s32 	%r22, %r25, 1;                         // %r22 = (blockIdx.y * blockDim.y + threadIdx.y) * unroll_size + 1
	mul.wide.s32 	%rd21, %r22, 4;                    // %rd21 = 
	add.s64 	%rd22, %rd18, %rd21;                   //
	atom.global.add.f32 	%f27, [%rd22], %f32;       // 
	add.s32 	%r23, %r25, 2;                         // 
	mul.wide.s32 	%rd23, %r23, 4;                    // %rd23 = 
	add.s64 	%rd24, %rd18, %rd23;                   // 
	atom.global.add.f32 	%f28, [%rd24], %f31;       // 
	add.s32 	%r24, %r25, 3;                         //
	mul.wide.s32 	%rd25, %r24, 4;                    // %rd25 = 
	add.s64 	%rd26, %rd18, %rd25;                   // 
	atom.global.add.f32 	%f29, [%rd26], %f30;       // 
	add.s64 	%rd28, %rd28, %rd27;                   // 
	add.s32 	%r25, %r3, %r25;                       // %r25 += %r3
	setp.lt.s32	%p4, %r25, %r9;                        // p4 = %r25 < size
	@%p4 bra 	BB0_2;                                 // if p4 -> goto BB0_2

BB0_5:
	ret;
}